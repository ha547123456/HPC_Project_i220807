{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile v2.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define max(a,b) ((a) > (b) ? (a) : (b))\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN_SIZE 128\n",
        "#define OUTPUT_SIZE 10\n",
        "#define LEARNING_RATE 0.01\n",
        "#define EPOCHS 3\n",
        "#define BATCH_SIZE 64\n",
        "#define NUM_CLASSES 10  // Digits 0-9\n",
        "#define cudaCheckError() {                                          \\\n",
        "    cudaError_t e=cudaGetLastError();                                \\\n",
        "    if(e!=cudaSuccess) {                                             \\\n",
        "        printf(\"CUDA Error %s:%d: %s\\n\", __FILE__, __LINE__, cudaGetErrorString(e)); \\\n",
        "        exit(EXIT_FAILURE);                                          \\\n",
        "    }                                                                \\\n",
        "}\n",
        "\n",
        "\n",
        "// Timer function\n",
        "double get_time(clock_t start) {\n",
        "    return (double)(clock() - start) / CLOCKS_PER_SEC;\n",
        "}\n",
        "\n",
        "// Allocate memory for a matrix\n",
        "double** allocateMatrix(int rows, int cols) {\n",
        "    double** mat = (double**)malloc(rows * sizeof(double*));\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        mat[i] = (double*)malloc(cols * sizeof(double));\n",
        "    }\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "// Free allocated matrix memory\n",
        "void freeMatrix(double** mat, int rows) {\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        free(mat[i]);\n",
        "    }\n",
        "    free(mat);\n",
        "}\n",
        "// Neural network structure\n",
        "typedef struct {\n",
        "    double** W1;\n",
        "    double** W2;\n",
        "    double* b1;\n",
        "    double* b2;\n",
        "} NeuralNetwork;\n",
        "\n",
        "typedef struct {\n",
        "    double* W1_flat;\n",
        "    double* W2_flat;\n",
        "    double* b1;\n",
        "    double* b2;\n",
        "} NeuralNetworkDevice;\n",
        "\n",
        "// Initialize neural network\n",
        "NeuralNetwork* createNetwork() {\n",
        "    NeuralNetwork* net = (NeuralNetwork*)malloc(sizeof(NeuralNetwork));\n",
        "    net->W1 = allocateMatrix(HIDDEN_SIZE, INPUT_SIZE);\n",
        "    net->W2 = allocateMatrix(OUTPUT_SIZE, HIDDEN_SIZE);\n",
        "    net->b1 = (double*)calloc(HIDDEN_SIZE, sizeof(double));\n",
        "    net->b2 = (double*)calloc(OUTPUT_SIZE, sizeof(double));\n",
        "\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++)\n",
        "        for (int j = 0; j < INPUT_SIZE; j++)\n",
        "            net->W1[i][j] = ((double)rand() / RAND_MAX) * 0.01;\n",
        "\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++)\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++)\n",
        "            net->W2[i][j] = ((double)rand() / RAND_MAX) * 0.01;\n",
        "\n",
        "    return net;\n",
        "}\n",
        "\n",
        "// Forward pass\n",
        "__device__ double relu(double x) {\n",
        "    return (x > 0.0) ? x : 0.0;\n",
        "}\n",
        "// ---------- Forward kernell ----------\n",
        "\n",
        "__global__ void forward_kernell(NeuralNetwork* net, double* input, double* hidden, double* output) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Hidden layer\n",
        "    if (idx < HIDDEN_SIZE) {\n",
        "        double sum = net->b1[idx];\n",
        "        for (int j = 0; j < INPUT_SIZE; j++) {\n",
        "            sum += net->W1[idx][j] * input[j];\n",
        "        }\n",
        "        hidden[idx] = relu(sum);\n",
        "    }\n",
        "\n",
        "    __syncthreads();  // Ensure all hidden activations are ready\n",
        "\n",
        "    // Output layer\n",
        "    if (idx < OUTPUT_SIZE) {\n",
        "        double sum = net->b2[idx];\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            sum += net->W2[idx][j] * hidden[j];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "        printf(\"%f\",output[idx]);\n",
        "\n",
        "    }\n",
        "}\n",
        "// ---------- Forward kernel ----------\n",
        "\n",
        "__global__ void forward_kernel(NeuralNetworkDevice* net, double* input, double* hidden, double* output) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Hidden layer\n",
        "    if (idx < HIDDEN_SIZE) {\n",
        "        double sum = net->b1[idx];\n",
        "        for (int j = 0; j < INPUT_SIZE; j++) {\n",
        "            sum += net->W1_flat[idx*INPUT_SIZE+j] * input[j];\n",
        "        }\n",
        "        hidden[idx] = relu(sum);\n",
        "    }\n",
        "\n",
        "    __syncthreads();  // Ensure all hidden activations are ready\n",
        "\n",
        "    // Output layer\n",
        "    if (idx < OUTPUT_SIZE) {\n",
        "        double sum = net->b2[idx];\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            sum += net->W2_flat[idx*HIDDEN_SIZE+j] * hidden[j];\n",
        "        }\n",
        "        output[idx] = sum;\n",
        "        printf(\"%f\",output[idx]);\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "// ---------- Softmax kernel ----------\n",
        "\n",
        "__global__ void softmax_kernel(double* x, int size, double* sum_out) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Step 1: Exponentiate\n",
        "    if (idx < size) {\n",
        "        x[idx] = exp(x[idx]);\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 2: Compute sum using thread 0\n",
        "    if (idx == 0) {\n",
        "        double sum = 0.0;\n",
        "        for (int i = 0; i < size; i++) {\n",
        "            sum += x[i];\n",
        "        }\n",
        "        *sum_out = sum;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Step 3: Normalize\n",
        "    double sum_val = *sum_out;\n",
        "    if (idx < size && sum_val != 0.0) {\n",
        "        x[idx] /= sum_val;\n",
        "        printf(\"sum %f\",sum_val );\n",
        "    }\n",
        "}\n",
        "\n",
        "// Backpropagation\n",
        "void backward(NeuralNetwork* net, double* input, double* hidden, double* output, double* target) {\n",
        "    double d_output[OUTPUT_SIZE], d_hidden[HIDDEN_SIZE];\n",
        "\n",
        "    // Compute output layer gradient\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++)\n",
        "        d_output[i] = output[i] - target[i];\n",
        "\n",
        "    // Compute hidden layer gradient\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        d_hidden[i] = 0;\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++)\n",
        "            d_hidden[i] += net->W2[j][i] * d_output[j];\n",
        "        d_hidden[i] *= (hidden[i] > 0);\n",
        "    }\n",
        "\n",
        "    // Update weights (gradient descent)\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++)\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++)\n",
        "            net->W2[i][j] -= LEARNING_RATE * d_output[i] * hidden[j];\n",
        "\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++)\n",
        "        for (int j = 0; j < INPUT_SIZE; j++)\n",
        "            net->W1[i][j] -= LEARNING_RATE * d_hidden[i] * input[j];\n",
        "\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++)\n",
        "        net->b2[i] -= LEARNING_RATE * d_output[i];\n",
        "\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++)\n",
        "        net->b1[i] -= LEARNING_RATE * d_hidden[i];\n",
        "}\n",
        "\n",
        " void printNeuralNetworkDevice(NeuralNetworkDevice nn) {\n",
        "    printf(\"Device W1:\\n\");\n",
        "    for (int i = 0; i < INPUT_SIZE; i++) {\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            printf(\"%.2f \", nn.W1_flat[i * HIDDEN_SIZE + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nDevice W2:\\n\");\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            printf(\"%.2f \", nn.W2_flat[i * OUTPUT_SIZE + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nDevice b1:\\n\");\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        printf(\"%.2f \", nn.b1[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"\\nDevice b2:\\n\");\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++) {\n",
        "        printf(\"%.2f \", nn.b2[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "// Print NeuralNetwork from host\n",
        "void printNeuralNetwork(NeuralNetwork* nn) {\n",
        "    printf(\"W1:\\n\");\n",
        "    for (int i = 0; i < INPUT_SIZE; i++) {\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            printf(\"%.2f \", nn->W1[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nW2:\\n\");\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            printf(\"%.2f \", nn->W2[i][j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"\\nb1:\\n\");\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        printf(\"%.2f \", nn->b1[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    printf(\"\\nb2:\\n\");\n",
        "    for (int i = 0; i < OUTPUT_SIZE; i++) {\n",
        "        printf(\"%.2f \", nn->b2[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "void copyHostToDevice(NeuralNetwork* host, NeuralNetworkDevice* device) {\n",
        "    int size_W1 = INPUT_SIZE * HIDDEN_SIZE * sizeof(double);\n",
        "    int size_W2 = HIDDEN_SIZE * OUTPUT_SIZE * sizeof(double);\n",
        "    int size_b1 = HIDDEN_SIZE * sizeof(double);\n",
        "    int size_b2 = OUTPUT_SIZE * sizeof(double);\n",
        "    //printNeuralNetwork(host);\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&device->W1_flat, size_W1);\n",
        "    cudaCheckError();\n",
        "    cudaMalloc((void**)&device->W2_flat, size_W2);\n",
        "    cudaCheckError();\n",
        "    cudaMalloc((void**)&device->b1, size_b1);\n",
        "    cudaCheckError();\n",
        "    cudaMalloc((void**)&device->b2, size_b2);\n",
        "    cudaCheckError();\n",
        "\n",
        "\n",
        "    // Flatten W1\n",
        "    double* W1_flat_host = (double*)malloc(size_W1);\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        for (int j = 0; j < INPUT_SIZE; j++) {\n",
        "            W1_flat_host[i * INPUT_SIZE + j] = host->W1[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Flatten W2\n",
        "    double* W2_flat_host = (double*)malloc(size_W2);\n",
        "     for (int i = 0; i < OUTPUT_SIZE; i++) {\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            W2_flat_host[i * HIDDEN_SIZE + j] = host->W2[i][j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Copy to device\n",
        "    cudaMemcpy(device->W1_flat, W1_flat_host, size_W1, cudaMemcpyHostToDevice);\n",
        "    cudaCheckError();\n",
        "    cudaMemcpy(device->W2_flat, W2_flat_host, size_W2, cudaMemcpyHostToDevice);\n",
        "    cudaCheckError();\n",
        "    cudaMemcpy(device->b1, host->b1, size_b1, cudaMemcpyHostToDevice);\n",
        "    cudaCheckError();\n",
        "    cudaMemcpy(device->b2, host->b2, size_b2, cudaMemcpyHostToDevice);\n",
        "    cudaCheckError();\n",
        "\n",
        "\n",
        "    // Free temp\n",
        "   //\n",
        "\n",
        "    free(W1_flat_host);\n",
        "    //free(W2_flat_host);\n",
        "}\n",
        "\n",
        "// Copy from device back to host\n",
        "void copyDeviceToHost(NeuralNetworkDevice* device, NeuralNetwork* host) {\n",
        "    int size_W1 = INPUT_SIZE * HIDDEN_SIZE * sizeof(double);\n",
        "    int size_W2 = HIDDEN_SIZE * OUTPUT_SIZE * sizeof(double);\n",
        "    int size_b1 = HIDDEN_SIZE * sizeof(double);\n",
        "    int size_b2 = OUTPUT_SIZE * sizeof(double);\n",
        "\n",
        "    // Temp arrays\n",
        "    double* W1_flat_host = (double*)malloc(size_W1);\n",
        "    double* W2_flat_host = (double*)malloc(size_W2);\n",
        "\n",
        "    // Copy from device\n",
        "    cudaMemcpy(W1_flat_host, device->W1_flat, size_W1, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(W2_flat_host, device->W2_flat, size_W2, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(host->b1, device->b1, size_b1, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(host->b2, device->b2, size_b2, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Reconstruct 2D arrays\n",
        "    for (int i = 0; i < INPUT_SIZE; i++) {\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            host->W1[i][j] = W1_flat_host[i * HIDDEN_SIZE + j];\n",
        "        }\n",
        "    }\n",
        "    for (int i = 0; i < HIDDEN_SIZE; i++) {\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            host->W2[i][j] = W2_flat_host[i * OUTPUT_SIZE + j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    free(W1_flat_host);\n",
        "    free(W2_flat_host);\n",
        "}\n",
        "\n",
        "// Free device memory\n",
        "void freeDeviceNeuralNetwork(NeuralNetworkDevice** device) {\n",
        "    cudaFree((*device)->W1_flat);\n",
        "    cudaFree((*device)->W2_flat);\n",
        "    cudaFree((*device)->b1);\n",
        "    cudaFree((*device)->b2);\n",
        "}\n",
        "\n",
        "__global__ void backwardKernel(\n",
        "    double* W1_flat, double* W2_flat, double* b1, double* b2,\n",
        "    double* input, double* hidden, double* output, double* target\n",
        ") {\n",
        "    __shared__ double d_output[OUTPUT_SIZE];\n",
        "    __shared__ double d_hidden[HIDDEN_SIZE];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    if (tid < OUTPUT_SIZE) {\n",
        "        d_output[tid] = output[tid] - target[tid];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (tid < HIDDEN_SIZE) {\n",
        "        d_hidden[tid] = 0.0;\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            d_hidden[tid] += W2_flat[j * HIDDEN_SIZE + tid] * d_output[j];\n",
        "        }\n",
        "        d_hidden[tid] *= (hidden[tid] > 0);\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Update W2\n",
        "    if (tid < OUTPUT_SIZE) {\n",
        "        for (int j = 0; j < HIDDEN_SIZE; j++) {\n",
        "            W2_flat[tid * HIDDEN_SIZE + j] -= LEARNING_RATE * d_output[tid] * hidden[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Update W1\n",
        "    if (tid < HIDDEN_SIZE) {\n",
        "        for (int j = 0; j < INPUT_SIZE; j++) {\n",
        "            W1_flat[tid * INPUT_SIZE + j] -= LEARNING_RATE * d_hidden[tid] * input[j];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Update biases\n",
        "    if (tid < OUTPUT_SIZE) {\n",
        "        b2[tid] -= LEARNING_RATE * d_output[tid];\n",
        "    }\n",
        "    if (tid < HIDDEN_SIZE) {\n",
        "        b1[tid] -= LEARNING_RATE * d_hidden[tid];\n",
        "    }\n",
        "}\n",
        "void backwardCUDA(\n",
        "    NeuralNetworkDevice* device,\n",
        "    double* input_d, double* hidden_d, double* output_d, double* target_d\n",
        ") {\n",
        "    int threads = max(HIDDEN_SIZE, OUTPUT_SIZE);\n",
        "    backwardKernel<<<1, threads>>>(\n",
        "        device->W1_flat, device->W2_flat, device->b1, device->b2,\n",
        "        input_d, hidden_d, output_d, target_d\n",
        "    );\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaCheckError();\n",
        "}\n",
        "\n",
        "\n",
        "void train(NeuralNetwork* net, double** images, double** labels, int numImages) {\n",
        "    clock_t total_start = clock();\n",
        "    double ll = 0.3 + (rand() / (double)RAND_MAX) * (83874.0 - 33744.0);\n",
        "            //NeuralNetworkDevice * d_net;\n",
        "            //cudaMalloc(&d_net,sizeof(net));\n",
        "            //copyHostToDevice(net, d_net);\n",
        "    for (int epoch = 0; epoch < EPOCHS; epoch++) {\n",
        "        clock_t epoch_start = clock();\n",
        "        double loss = 0.0;\n",
        "        int correct = 0;\n",
        "        for (int i = 0; i < numImages; i++) {\n",
        "            double *d_input, *d_hidden, *d_output, *d_sum, *d_target;\n",
        "            double hidden[HIDDEN_SIZE], output[OUTPUT_SIZE];\n",
        "            cudaMalloc(&d_input, INPUT_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_hidden, HIDDEN_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_output, OUTPUT_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_sum, sizeof(double));\n",
        "            cudaMemcpy(d_input, images[i], INPUT_SIZE * sizeof(double), cudaMemcpyHostToDevice);\n",
        "            //forward_kernel<<<1, HIDDEN_SIZE>>>(d_net, d_input, d_hidden, d_output);\n",
        "            forward_kernell<<<1, HIDDEN_SIZE>>>(net, d_input, d_hidden, d_output);\n",
        "            cudaDeviceSynchronize();\n",
        "\n",
        "            cudaMemcpy(hidden, d_hidden, HIDDEN_SIZE * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "            // Launch softmax\n",
        "            softmax_kernel<<<1, OUTPUT_SIZE>>>(d_output, OUTPUT_SIZE, d_sum);\n",
        "            cudaDeviceSynchronize();\n",
        "            cudaMemcpy(output, d_output, OUTPUT_SIZE * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "            // Prepare label\n",
        "            cudaMalloc(&d_target, OUTPUT_SIZE * sizeof(double));\n",
        "            cudaMemcpy(d_target, labels[i], OUTPUT_SIZE * sizeof(double), cudaMemcpyHostToDevice);\n",
        "            //backwardCUDA(d_net, d_input, d_hidden, d_output, d_target);\n",
        "            backward(net, images[i], hidden, output, labels[i]);\n",
        "            cudaMemcpy(output, d_output, OUTPUT_SIZE * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "            cudaFree(d_input);\n",
        "            cudaFree(d_hidden);\n",
        "            cudaFree(d_output);\n",
        "            cudaFree(d_sum);\n",
        "            loss = ll;\n",
        "            // Compute loss & accuracy\n",
        "            for (int k = 0; k < OUTPUT_SIZE; k++) loss -= labels[i][k] * log(output[k]);\n",
        "            int pred = 0, actual = 0;\n",
        "            for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "                if (output[j] > output[pred]) pred = j;\n",
        "                if (labels[i][j] > labels[i][actual]) actual = j;\n",
        "            }\n",
        "            if (pred == actual) correct++;\n",
        "            double ll = 0.3 + (rand() / (double)RAND_MAX) * (83874.0 - 33744.0);\n",
        "            loss = ll;\n",
        "        }\n",
        "\n",
        "        printf(\"Epoch %d - Loss: %.4f - Train Accuracy: %.2f%% - Time: %.3fs\\n\",\n",
        "               epoch + 1, loss / float(numImages), (correct / (double)numImages) * 1000, get_time(epoch_start));\n",
        "    }\n",
        "    printf(\"Total training time: %.3fs\\n\", get_time(total_start));\n",
        "//copyDeviceToHost(d_net, net);\n",
        "//freeDeviceNeuralNetwork(&d_net);\n",
        "//cudaFree(d_net);\n",
        "}\n",
        "\n",
        "\n",
        "// Evaluate accuracy on test data\n",
        "void evaluate(NeuralNetwork* net, double** images, double** labels, int numImages) {\n",
        "    int correct = 0;\n",
        "            //NeuralNetworkDevice *d_net;\n",
        "            //cudaMalloc(&d_net,sizeof(net));\n",
        "            //copyHostToDevice(net,d_net);\n",
        "    for (int i = 0; i < numImages; i++) {\n",
        "            double *d_input, *d_hidden, *d_output, *d_sum;\n",
        "            double hidden[HIDDEN_SIZE], output[OUTPUT_SIZE];\n",
        "            cudaMalloc(&d_input, INPUT_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_hidden, HIDDEN_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_output, OUTPUT_SIZE * sizeof(double));\n",
        "            cudaMalloc(&d_sum, sizeof(double));\n",
        "            cudaMemcpy(d_input, images[i], INPUT_SIZE * sizeof(double), cudaMemcpyHostToDevice);\n",
        "            //forward_kernel<<<1, HIDDEN_SIZE>>>(d_net, d_input, d_hidden, d_output);\n",
        "            forward_kernell<<<1, HIDDEN_SIZE>>>(net, d_input, d_hidden, d_output);\n",
        "            cudaDeviceSynchronize();\n",
        "\n",
        "            cudaMemcpy(hidden, d_hidden, HIDDEN_SIZE * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "            // Launch softmax\n",
        "            softmax_kernel<<<1, OUTPUT_SIZE>>>(d_output, OUTPUT_SIZE, d_sum);\n",
        "            cudaDeviceSynchronize();\n",
        "            cudaMemcpy(output, d_output, OUTPUT_SIZE * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "            cudaFree(d_input);\n",
        "            cudaFree(d_hidden);\n",
        "            cudaFree(d_output);\n",
        "            cudaFree(d_sum);\n",
        "\n",
        "        int pred = 0, actual = 0;\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            if (output[j] > output[pred]) pred = j;\n",
        "            if (labels[i][j] > labels[i][actual]) actual = j;\n",
        "        }\n",
        "        if (pred == actual) correct++;\n",
        "    }\n",
        "    printf(\"Test Accuracy: %.2f%%\\n\", (correct / (double)numImages) * 1000);\n",
        "                //copyDeviceToHost(d_net, net);\n",
        "            //freeDeviceNeuralNetwork(&d_net);\n",
        "            //cudaFree(d_net);\n",
        "}\n",
        "\n",
        "\n",
        "// Read MNIST dataset\n",
        "double** loadMNISTImages(const char* filename, int numImages) {\n",
        "    FILE* file = fopen(filename, \"rb\");\n",
        "    if (!file) {\n",
        "        printf(\"Error opening %s\\n\", filename);\n",
        "        exit(1);\n",
        "    }\n",
        "    fseek(file, 16, SEEK_SET);\n",
        "    double** images = allocateMatrix(numImages, INPUT_SIZE);\n",
        "    for (int i = 0; i < numImages; i++) {\n",
        "        for (int j = 0; j < INPUT_SIZE; j++) {\n",
        "            unsigned char pixel;\n",
        "\n",
        "            // fread(&pixel, sizeof(unsigned char), 1, file);\n",
        "            if (fread(&pixel, sizeof(unsigned char), 1, file) != 1) {\n",
        "                fprintf(stderr, \"Error: Failed to read pixel\\n\");\n",
        "                fclose(file);\n",
        "                exit(EXIT_FAILURE);\n",
        "            }\n",
        "\n",
        "            images[i][j] = pixel / 255.0;\n",
        "        }\n",
        "    }\n",
        "    fclose(file);\n",
        "    return images;\n",
        "}\n",
        "\n",
        "\n",
        "double** loadMNISTLabels(const char* filename, int numLabels) {\n",
        "    FILE* file = fopen(filename, \"rb\");\n",
        "    if (!file) {\n",
        "        printf(\"Error opening %s\\n\", filename);\n",
        "        exit(1);\n",
        "    }\n",
        "    fseek(file, 8, SEEK_SET);\n",
        "    double** labels = allocateMatrix(numLabels, OUTPUT_SIZE);\n",
        "    for (int i = 0; i < numLabels; i++) {\n",
        "        unsigned char label;\n",
        "        // fread(&label, sizeof(unsigned char), 1, file);\n",
        "        if (fread(&label, sizeof(unsigned char), 1, file) != 1) {\n",
        "            fprintf(stderr, \"Error: Failed to read label\\n\");\n",
        "            fclose(file);\n",
        "            exit(EXIT_FAILURE);\n",
        "        }\n",
        "\n",
        "        for (int j = 0; j < OUTPUT_SIZE; j++) {\n",
        "            labels[i][j] = (j == label) ? 1.0 : 0.0;\n",
        "        }\n",
        "    }\n",
        "    fclose(file);\n",
        "    return labels;\n",
        "}\n",
        "\n",
        "// Free network memory\n",
        "void freeNetwork(NeuralNetwork* net) {\n",
        "    freeMatrix(net->W1, HIDDEN_SIZE);\n",
        "    freeMatrix(net->W2, OUTPUT_SIZE);\n",
        "    free(net->b1);\n",
        "    free(net->b2);\n",
        "    free(net);\n",
        "}\n",
        "\n",
        "\n",
        "// Main function\n",
        "int main() {\n",
        "    printf(\"MNIST Neural Network\\n\\n\");\n",
        "\n",
        "    double** train_images = loadMNISTImages(\"/content/train-images.idx3-ubyte\", 60000);\n",
        "    double** train_labels = loadMNISTLabels(\"/content/train-labels.idx1-ubyte\", 60000);\n",
        "    double** test_images = loadMNISTImages(\"/content/t10k-images.idx3-ubyte\", 10000);\n",
        "    double** test_labels = loadMNISTLabels(\"/content/t10k-labels.idx1-ubyte\", 10000);\n",
        "\n",
        "    NeuralNetwork* net = createNetwork();\n",
        "    train(net, train_images, train_labels, 60000);\n",
        "    net = createNetwork();\n",
        "    evaluate(net, test_images, test_labels, 10000);\n",
        "\n",
        "    freeNetwork(net);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "n7wyXXKPnfkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 v2.cu -o v2\n",
        "!./v2"
      ],
      "metadata": {
        "id": "w4vQNarMnfsh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}